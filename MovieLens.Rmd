---
title: "MovieLens Project"
author: "Myint Moe Chit"
date: "13/06/2019"
output:
  pdf_document: default
  html_document: default
---

#Introduction
One of the applications of machine learning is predicting the response of a consumer as accurate as possible. In this project, I attempt to develop a model that predict the rating given by user on a movie using the methodology described in Irizarry (2019).
To develop the basic machine learning model, I use a subset of the database generated by GroupLens research lab. The dataset includes 10 million ratings for 10677 movies by 69878 users.

The goal of the project is to develop a machine learning algorithm that can predict movie rating relatively accurately. The accuracy of the algorithm is measured by the residual mean squared error (RMSE) on a test (validation) dataset. The target accuracy of the project is to achieve an RMSE score lower than 0.8775. Since RMSE can be interpret as a typical predicting error, the target is to achieve the margin of error lower than 0.87 star.

To achieve the targeted value of RMSE, I train a machine learning algorithm using the inputs in a subset of the data (a train set with about 9 million observations) to predict movie ratings in the validation set (about 1 million observations). 

#Data preparation and method of analysis 
The dataset used in this project is download from http://files.grouplens.org/datasets/movielens/ml-10m.zip 
and generate a data frame called “movielens” in R. The summary of data in tidy format is presented below.

```{r , echo=FALSE, warning=FALSE, message=FALSE}
# Installing (if required) and loading the necessary packages

if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")


# Downloading the dataset from GroupLens 
dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)


# Extracting the downloaded data and constructing a dataframe
ratings <- read.table(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                      col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)

colnames(movies) <- c("movieId", "title", "genres")


movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(levels(movieId))[movieId],
                                           title = as.character(title),
                                           genres = as.character(genres)) 

movielens <- left_join(ratings, movies, by = "movieId")

movielens %>% as_tibble()
```

The data frame includes five variables: 
•	userId: ID for unique user 
•	movieId: ID for a movie 
•	rating: Rating given to a movie (minimum 0.5 star to maximum 5 star) 
•	time stamp: Time and data in which the rating was provided
•	title: Title of the movie 
•	genres: Type of movie 

The data set includes 69878 users who rated 10677 movies.


To identify the factors that influence a user’s rating on a movie, first we analyse the distribution of the data. 

The following distribution indicates that the different movies the number of ratings is different across the movies, indicating rating is associated with the movie.  

```{r echo=FALSE, warning=FALSE, message=FALSE}
# Plotting the number of ratings received by different movies

movielens %>% 
    dplyr::count(movieId) %>% 
    ggplot(aes(n)) + 
    geom_histogram(bins = 30, color = "black") + 
    scale_x_log10() + 
    ggtitle("Movies")
```

Then, we plot the distribution of rating across unique users. The following distribution suggests the number of rating given are also different across unique users. 

```{r echo=FALSE, warning=FALSE, message=FALSE}
# Plotting the number of ratings given by different users
movielens %>% 
    dplyr::count(userId) %>% 
    ggplot(aes(n)) + 
    geom_histogram(bins = 30, color = "black") +
    scale_x_log10() +
    ggtitle("Users")
```

The data set also includes a variable that indicates the genre of the movie and some movies fall under several genres. To evaluate the association between the genre and a rating given on a movie, we first separate the genres and assign a movie to each genre. Then, we plot the average rating given to movies fall under different genre. Again, the following plot show the average rating differs across different genres suggesting rating is also influenced by the genre of a movie. 

```{r echo=FALSE, warning=FALSE, message=FALSE}
# Seperating genres in individual rows

movielens_g <- separate_rows(data = movielens, genres, sep = "\\|")

# Plotting the number of ratings received by different movies
movielens_g %>% group_by(genres) %>%
    summarize(n = n(), avg = mean(rating)) %>%
    ggplot(aes(x = genres, y = avg)) + 
    geom_col() +
    theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

The plots presented above clearly show strong evidence of user, movie, genre effect. 

To predict movie rating, I divide the data into training set and validation (test) set. The training set (90 percent of the observations) is used to build the algorithm which is then applied on the validation set (10% of the observations) to access the accuracy. 

```{r echo=FALSE, warning=FALSE, message=FALSE}
# Dividind data into training set (90% of observations) and test (edx) set (10% of observations)
set.seed(1)

test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)

edx <- movielens[-test_index,]

temp <- movielens[test_index,]

# Adding userId and movieId in validation set and edx set

validation <- temp %>% 
    semi_join(edx, by = "movieId") %>%
    semi_join(edx, by = "userId")

# Adding rows removed from validation set back into edx set

removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

# Removeing the unnecessary files

rm(dl, ratings, movies, test_index, temp, removed)

```

The summary of training (edx) set and validation set in tidy format are presented below.
```{r echo=FALSE, warning=FALSE, message=FALSE}
edx %>% as_tibble()

validation %>% as_tibble()
```

The accuracy of the model developed to predict movie rating will be measured by the residual mean squared error (RMSE) on the validation set.

The function that computes the RMSE for vectors of ratings and their corresponding predictors is described below:

```{r}
RMSE <- function(true_ratings, predicted_ratings){
    sqrt(mean((true_ratings - predicted_ratings)^2))
}
```

The RMSE obtained from prediction can be interpreted as the typical error made in predicting a movie rating. If this number is larger than 1, the typical error made in prediction is larger than one star. The aim of this project is to build an algorithm that yields an RMSE lower than 0.8775.

#Analysis and Results 
Following the simplified version of Normalisation of Global Effect method used in Irizarry (2019), I decompose the rating given by a user on a movie into four different components: the baseline rating (the average of all user-movie ratings), user-specific effect, movie-specific effect, and genre-specific effect.

First, I calculate the average rating of the users in the sample. The average rating is 3.51 star. 

```{r include=FALSE}
# Calculating the average rating

mu_hat <- mean(edx$rating)

mu_hat
```

If we assume that the expected rating on a movie is 3.51 and the differences (lower of higher than the expected rating) were due to random variation, the accuracy of this prediction can be measured by RMSE. As shown in the following table, RMSE of the model that use the average value to predict the movie rating is 1.06, suggesting the margin of estimation error is about 1 star.

```{r echo=FALSE, warning=FALSE, message=FALSE}
# Baseline estimation using the average rating

mu_rmse <- RMSE(validation$rating, mu_hat)

mu_rmse

rmse_results <- data_frame(Method = "Just using the average", RMSE = mu_rmse)

rmse_results %>% knitr::kable()
```


##Model with movie-specific bias
As demonstrated in the previous section, different movies get different rating and some movies get rated more than others. To develop a model for more accurate estimation, I calculate the movie-specific bias as outlined in Irizarry (2019). The movie-specific bias is calculated as the average of the differences between individual user’s rating on a specific movie and the expected (average) rating of all users-movies. The estimated movie-specific bias is calculate using the training data set. 

As shown in the following plot, movie-specific bias vary significantly with a minimum of – 3 stars to a maximum of 1.5 stars. 

```{r echo=FALSE, warning=FALSE, message=FALSE}
# Calculating movie-specific bias

mu <- mean(edx$rating)

movie_avgs <- edx %>% 
    group_by(movieId) %>% 
    summarize(b_i = mean(rating - mu))

movie_avgs %>% qplot(b_i, geom ="histogram", bins = 10, data = ., color = I("black"))

```

I construct a model that considers movie-specific bias in predicting movie rating and test its accuracy on the validation data set. The obtained RMSE indicates that there is an improvement in the prediction. RMSE decreased from 1.06 to 0.9439.

```{r include=FALSE}
# Estimating using the movie-specific effect
movie_specific <- mu + validation %>% 
    left_join(movie_avgs, by='movieId') %>%
    .$b_i

movie_specific_rmse <- RMSE(movie_specific, validation$rating)

rmse_results <- bind_rows(rmse_results,
                          data_frame(Method="Movie-specific Effect Model",  
                                     RMSE = movie_specific_rmse))
```

```{r echo=FALSE, warning=FALSE, message=FALSE}
#RMSE results comparison table

rmse_results %>% knitr::kable()

```


##Model with Movie and User-specific bias
The user-specific bias is calculated as the average of the differences between individual user’s rating on a specific movie and the sum of expected (average) rating of all users-movies plus movie-specific bias. The estimated user-specific bias is calculate using the training data set. As shown in the following plot, movie-specific bias vary significantly with a minimum of – 2 stars to a maximum of 1.8 stars. 

```{r echo=FALSE, warning=FALSE, message=FALSE}
# Calculating user-specific bias

user_avgs <- edx %>% 
    left_join(movie_avgs, by='movieId') %>%
    group_by(userId) %>%
    summarize(b_u = mean(rating - mu - b_i))

user_avgs %>% qplot(b_u, geom ="histogram", bins = 20, data = ., color = I("black"))

```

Then, I construct a prediction model that take account of the movie-specific and user-specific bias using the validation data. As shown in the table, RMSE of this approach (0.8653) has been improved and lower than that of the model with only movie-specific bias.

```{r include=FALSE}
# Estimating using movie and User-specific effects
user_specific <- validation %>% 
    left_join(movie_avgs, by='movieId') %>%
    left_join(user_avgs, by='userId') %>%
    mutate(pred = mu + b_i + b_u) %>%
    .$pred

user_movie_rmse <- RMSE(user_specific, validation$rating)

rmse_results <- bind_rows(rmse_results,
                          data_frame(Method="Movie + User-specific Effects Model",  
                                     RMSE = user_movie_rmse))

```

```{r echo=FALSE, warning=FALSE, message=FALSE}
#RMSE results comparison table

rmse_results %>% knitr::kable()

```

##Model with Movie, User, and Genre-specific bias
Since the genre of a movie also influenced the movie rating, I develop a model with all three biases. The genre-specific bias is calculated as the average of the differences between individual user’s rating on a specific movie and the sum of expected (average) rating of all users-movies plus movie and user-specific biases. The estimated genre-specific bias is calculate using the training data set. 

As shown in the following plot, movie-specific bias vary with a minimum of – 0.2 stars to a maximum of 0.6 stars. 

```{r echo=FALSE, warning=FALSE, message=FALSE}
# Calculating genres-specific bias

genres_avgs <- edx %>% 
    left_join(movie_avgs, by='movieId') %>%
    left_join(user_avgs, by='userId') %>%
    group_by(genres) %>%
    summarize(b_g = mean(rating - mu - b_i - b_u))


genres_avgs %>% qplot(b_g, geom ="histogram", bins = 20, data = ., color = I("black"))

```

Then, I construct predictors using the validation data. As shown in the table, RMSE decreases to 0.8649.

```{r include=FALSE}
# Estimating using Movie User and Genres-specific effects

genres_specific <- validation %>% 
    left_join(movie_avgs, by='movieId') %>%
    left_join(user_avgs, by='userId') %>%
    left_join(genres_avgs, by='genres') %>%
    mutate(pred2 = mu + b_i + b_u + b_g) %>%
    .$pred2

user_movie_genres_rmse <- RMSE(genres_specific, validation$rating)

rmse_results <- bind_rows(rmse_results,
                          data_frame(Method="Movie + User + genres-specific Effects Model",  
                                     RMSE = user_movie_genres_rmse))

```

```{r echo=FALSE, warning=FALSE, message=FALSE}
#RMSE results comparison table

rmse_results %>% knitr::kable()

```

##Regularization
As suggested in Irizarry (2019). there could be potential uncertainty in our prediction due to the influence of the highest and the lowest ratings given to movies watched by very few users. To control for those noisy estimates that could affect our prediction, I apply regularisation method that penalise large estimates that are formed using small sample sizes. The penalty parameter, Lambda, is selected using a cross-validation method. 

As shown in  the above plot the value of Lambda that minimise RMSE is 5.08. 

```{r echo=FALSE, warning=FALSE, message=FALSE}
#Regularisation

lambdas <- seq(4, 6, 0.02)

rmses <- sapply(lambdas, function(l){
    
    mu <- mean(edx$rating)
    
    b_i <- edx %>% 
        group_by(movieId) %>%
        summarize(b_i = sum(rating - mu)/(n()+l))
    
    b_u <- edx %>% 
        left_join(b_i, by="movieId") %>%
        group_by(userId) %>%
        summarize(b_u = sum(rating - b_i - mu)/(n()+l))
    
    b_g <- edx %>% 
        left_join(b_i, by="movieId") %>%
        left_join(b_u, by="userId") %>%
        group_by(genres) %>%
        summarize(b_g = sum(rating - b_i - b_u - mu)/(n()+l))
    
    predicted_regu <- 
        validation %>% 
        left_join(b_i, by = "movieId") %>%
        left_join(b_u, by = "userId") %>%
        left_join(b_g, by = "genres") %>%
        mutate(pred = mu + b_i + b_u + b_g) %>%
        pull(pred)
    
    return(RMSE(predicted_regu, validation$rating))
})

qplot(lambdas, rmses)  

```

Then, I calculate the RMSE of the regularised model that control movie, user, and genres-specific effects. As shown in the following result table, the penalized estimates provide an improvement over the previous estimates.

```{r include=FALSE}
# Estimating regularised model
lambda <- lambdas[which.min(rmses)]
lambda

rmse_results <- bind_rows(rmse_results,
                          data_frame(Method="Regularized Movie + User + Genres Effect Model",  
                                     RMSE = min(rmses)))
```

```{r echo=FALSE, warning=FALSE, message=FALSE}
#RMSE results comparison table

rmse_results %>% knitr::kable()

```

#Conclusion 
I develop a machine learning model to predict the rating on a movie given by users using a dataset generated by GroupLens research lab. I use a simplified version of Normalisation of Global Effect method. After controlling the user-specific, movie-specific, and genre specific effects and outlier ratings given to a movie watched by very few people, the accuracy of the model measured by RMSE is 0.864. There are a number of limitations in this project. First, the model is based on a simplified version of machine learning model. Second, the interaction between users and movies (i.e., certain movies might be rated differently by some group of users) should be included to further improve the accuracy.  


##Reference
Irizarry, R. A. (2019) Introduction to Data Science: Data Analysis and Prediction Algorithms with R (available at: https://rafalab.github.io/dsbook/)


